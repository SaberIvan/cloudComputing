{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "<labCode>-<hgao6048>-<Hanzhe>-<Gao>.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Load all needed library"
      ],
      "metadata": {
        "id": "_y5_jstSaay2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspark\n",
        "import string\n",
        "import nltk\n",
        "import re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7rHRwfMF_p7",
        "outputId": "f0787a19-bbc0-40df-f719-d4b99267cac7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.2.1.tar.gz (281.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 281.4 MB 30 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9.3\n",
            "  Downloading py4j-0.10.9.3-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 50.1 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.2.1-py2.py3-none-any.whl size=281853642 sha256=434001cbd9db1bd2a4dfec053a94fe59d049e7a1124aa511df73d974f9c58591\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/f5/07/7cd8017084dce4e93e84e92efd1e1d5334db05f2e83bcef74f\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9.3 pyspark-3.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession \\\n",
        "    .builder \\\n",
        "    .appName(\"COMP5349 A1\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "qz834jVDaq41"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Initial text processing(same in algorithm1 and algorithm2)\n",
        "\n",
        "1. Remove (Page xx):\n",
        "*   In the sentence: re.split(\"\\(Page \\d\\)\")\n",
        "*   At the end of the sentence: cut the context from the beginning of a sentence to 9 index befor the end of the sentence.(sentence[:-9])\n",
        "\n",
        "2. Split phrase to keywords:\n",
        "*   Replace stopwords & punctuations to '|', then split by it.\n",
        "*   Choose keyword length between 0 to 4 and return.\n",
        "\n"
      ],
      "metadata": {
        "id": "FILZKPTzgILv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Algorithm 1"
      ],
      "metadata": {
        "id": "9IcdidS69aYY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**step 1**:Using a dictionary to calculate the single word score.\n",
        "\n",
        "**step 2**:Using another dictionary to calculate keywords socre.\n",
        "\n",
        "**step 3**:Sorted by keywords score, the top four scores can return (keywords, 1), the other phrase return (keywords, 0).If there are less than four keywords in a phrase, all the element return (keywords, 1). This number means edf value.\n",
        "\n",
        "result for step3: (keywords, edf)\n",
        "\n",
        "**step 4 & 5**: Through agreegateByKey() calculate total edf and rdf where the rdf automatically increments by one each time it is traversed.Then using mapValues() calculate essentiality.\n",
        "\n",
        "Intermediate results(step 4)：(keywords, edf, rdf)\n",
        "\n",
        "Result for step5: (keywords, essentiality) (essentiality = edf(k)/rdf(k)*edf(k))\n",
        "\n",
        "**step 6**: Sorted by essentiality and list top 20."
      ],
      "metadata": {
        "id": "ZAkd7avJfZwf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords as sw\n",
        "\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def isPunct(word):\n",
        "  return len(word) == 1 and word in string.punctuation\n",
        "\n",
        "def separate_phrase(documents):\n",
        "  phrase_list = []\n",
        "  for document in documents:\n",
        "    sww = sw.words()\n",
        "    document = map(lambda x: \"|\" if x in sww else x,\n",
        "          word_tokenize(document.lower()))\n",
        "    phrase = []\n",
        "    for keywords in document:\n",
        "        if keywords == \"|\" or isPunct(keywords):  \n",
        "          if len(phrase) > 0 and len(phrase) < 5:\n",
        "            phrase_list.append(phrase)\n",
        "            phrase = []\n",
        "          elif len(phrase) > 4:\n",
        "            # phrase_list.append(phrase[0:4])\n",
        "            phrase = []\n",
        "        else:\n",
        "          phrase.append(keywords)\n",
        "    return phrase_list\n",
        "\n",
        "def calculate_word_scores(phrase_list):\n",
        "    word_freq = nltk.FreqDist()\n",
        "    word_degree = nltk.FreqDist()\n",
        "    for phrase in phrase_list:\n",
        "      degree = len(phrase) - 1\n",
        "      for word in phrase:\n",
        "        word_freq[word] += 1\n",
        "        word_degree[word] += degree # other words\n",
        "    for word in word_freq.keys():\n",
        "      word_degree[word] = word_degree[word] + word_freq[word] # itself\n",
        "    # word score = deg(w) / freq(w)\n",
        "    word_scores = {}\n",
        "    for word in word_freq.keys():\n",
        "      word_scores[word] = word_degree[word] / word_freq[word]\n",
        "    return word_scores\n",
        "\n",
        "def generate_candidate_keyword_scores(phrase_list, word_score):\n",
        "  phrase_scores = {}\n",
        "  for phrase in phrase_list:\n",
        "    phrase_score = 0\n",
        "    for word in phrase:\n",
        "      phrase_score += word_score[word]\n",
        "    phrase_scores[\" \".join(phrase)] = phrase_score\n",
        "  return phrase_scores\n",
        "\n",
        "def generate_keyword_edf_pair(keyword_candidates):\n",
        "  if(len(keyword_candidates)==0):\n",
        "    return []\n",
        "  elif(len(keyword_candidates)< 4):\n",
        "    for phrase in keyword_candidates:\n",
        "      return [(word, 1) for word in phrase]\n",
        "  else: \n",
        "    keyword_candidates_order =sorted(keyword_candidates.items(),key=lambda x:x[1],reverse=True)\n",
        "    keyword_candidates_getedf = [(wordScore[0], 1) for wordScore in keyword_candidates_order[:4]]\n",
        "    keyword_candidates_noedf = [(wordScore[0], 0) for wordScore in keyword_candidates_order[4:]]\n",
        "    result_candidates = keyword_candidates_getedf + keyword_candidates_noedf\n",
        "    return [edf_rdf_pair for edf_rdf_pair in result_candidates]\n"
      ],
      "metadata": {
        "id": "Zuk-WBY89cv2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "446b7b51-a9aa-4506-8dbd-9e94d385c1e4"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getEdfPairFromGoverningLaw (row):\n",
        "  row_dict = row.asDict()\n",
        "  if not row_dict['Governing Law']:\n",
        "    return []\n",
        "  else:\n",
        "    governingLaw = row_dict['Governing Law']\n",
        "    documents = re.split(pattern=\"\\(Page \\d\\)\", string=governingLaw[:-9].strip())\n",
        "    phrase_list = separate_phrase(documents)\n",
        "    word_score = calculate_word_scores(phrase_list)\n",
        "    keyword_candidates = generate_candidate_keyword_scores(phrase_list, word_score)\n",
        "    result_candidates = generate_keyword_edf_pair(keyword_candidates)\n",
        "    return result_candidates\n",
        "\n",
        "def getEdfPairFromAntiAssignment (row):\n",
        "  row_dict = row.asDict()\n",
        "  if not row_dict[\"Anti-assignment\"]:\n",
        "    return []\n",
        "  else:\n",
        "    antiAssignment = row_dict['Anti-assignment'] \n",
        "    documents = re.split(pattern=\"\\(Page \\d\\)\", string=antiAssignment[:-9].strip())\n",
        "    phrase_list = separate_phrase(documents)\n",
        "    word_score = calculate_word_scores(phrase_list)\n",
        "    keyword_candidates = generate_candidate_keyword_scores(phrase_list, word_score)\n",
        "    result_candidates = generate_keyword_edf_pair(keyword_candidates)\n",
        "    return result_candidates\n",
        "\n",
        "def getEdfPairFromChangeOfControl (row):\n",
        "  row_dict = row.asDict()\n",
        "  if not row_dict['Change of Control']:\n",
        "    return []\n",
        "  else:\n",
        "    changeOfControl = row_dict['Change of Control']\n",
        "    documents = re.split(pattern=\"\\(Page \\d\\)\", string=changeOfControl[:-9].strip())\n",
        "    phrase_list = separate_phrase(documents)\n",
        "    word_score = calculate_word_scores(phrase_list)\n",
        "    keyword_candidates = generate_candidate_keyword_scores(phrase_list, word_score)\n",
        "    result_candidates = generate_keyword_edf_pair(keyword_candidates)\n",
        "    return result_candidates"
      ],
      "metadata": {
        "id": "8YQaJ-od6s_n"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def seqFunc(accumulatedPair,singleEdf): \n",
        "  edf, rdf = accumulatedPair\n",
        "  edf += singleEdf\n",
        "  rdf += 1\n",
        "  return (edf, rdf)\n",
        "def combFunc(accumulatedPair1,accumulatedPair2):\n",
        "  edf1, rdf1 = accumulatedPair1\n",
        "  edf2, rdf2 = accumulatedPair2\n",
        "  return(edf1 + edf2, rdf1 + rdf2)\n",
        "\n",
        "def essentiality(value):\n",
        "  edf, rdf = value\n",
        "  essentiality = float(edf/rdf)*edf\n",
        "  return essentiality"
      ],
      "metadata": {
        "id": "smw3LpVI_YvL"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rddG = spark.read.csv(\"Governing_Law.csv\",header=True).rdd\n",
        "edf_keywordsPair_GoverningLaw = rddG.flatMap(getEdfPairFromGoverningLaw)\n",
        "edf_keywordsPair_GoverningLaw.aggregateByKey((0,0), seqFunc, combFunc, 1)\\\n",
        "              .mapValues(essentiality)\\\n",
        "              .sortBy(lambda x:x[1],ascending=False).take(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3xzX0FUZ4w3",
        "outputId": "08ba2bee-6e53-4a0c-d4c7-12e1538448ad"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('agreement shall', 201.98360655737704),\n",
              " ('new york', 53.38888888888889),\n",
              " ('without regard', 39.0625),\n",
              " ('without giving effect', 37.0),\n",
              " ('governed', 36.608450704225355),\n",
              " ('agreement', 29.308139534883722),\n",
              " ('without reference', 19.36),\n",
              " ('construed', 18.0625),\n",
              " ('united states', 17.391304347826086),\n",
              " ('law principles', 16.025641025641026),\n",
              " ('laws principles', 13.78125),\n",
              " ('law provisions', 13.473684210526315),\n",
              " ('substantive laws', 12.8),\n",
              " ('law rules', 11.25),\n",
              " ('laws provisions', 9.941176470588236),\n",
              " ('laws', 9.651474530831099),\n",
              " ('performed entirely within', 9.0),\n",
              " ('new york without regard', 9.0),\n",
              " ('internal laws', 8.521739130434783),\n",
              " ('laws provisions thereof', 8.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rddA = spark.read.csv(\"Anti_assignment_CIC_g3.csv\",header=True).rdd\n",
        "edf_keywordsPair_Anti = rddA.flatMap(getEdfPairFromAntiAssignment)\n",
        "edf_keywordsPair_Anti.aggregateByKey((0,0), seqFunc, combFunc, 1)\\\n",
        "              .mapValues(essentiality)\\\n",
        "              .sortBy(lambda x:x[1],ascending=False).take(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVr-KwCOZ5-7",
        "outputId": "0aa4ed43-dc63-47e3-9317-8d123caaad96"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('prior written consent', 170.273631840796),\n",
              " ('neither party may assign', 48.0),\n",
              " ('either party without', 21.77777777777778),\n",
              " ('either party may assign', 21.0),\n",
              " ('neither party shall assign', 18.0),\n",
              " ('agreement may', 17.163265306122447),\n",
              " ('a', 13.0),\n",
              " ('neither party shall', 13.0),\n",
              " ('e', 13.0),\n",
              " ('agreement without', 11.571428571428571),\n",
              " ('obligations hereunder without', 11.529411764705882),\n",
              " ('s', 11.0),\n",
              " ('express written consent', 10.0),\n",
              " ('party may assign', 9.38888888888889),\n",
              " ('t', 9.0),\n",
              " ('third party without', 8.642857142857142),\n",
              " ('express prior written consent', 8.0),\n",
              " (\"party 's written consent\", 8.0),\n",
              " ('prior written approval', 7.111111111111111),\n",
              " ('n', 7.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rddA = spark.read.csv(\"Anti_assignment_CIC_g3.csv\",header=True).rdd\n",
        "edf_keywordsPair_Anti = rddA.flatMap(getEdfPairFromChangeOfControl)\n",
        "edf_keywordsPair_Anti.aggregateByKey((0,0), seqFunc, combFunc, 1)\\\n",
        "              .mapValues(essentiality)\\\n",
        "              .sortBy(lambda x:x[1],ascending=False).take(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OdyCSswZ6wM",
        "outputId": "c32f16a4-2fed-4bb5-f608-152097586025"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('prior written consent', 7.6923076923076925),\n",
              " ('days written notice', 3.0),\n",
              " ('licensor may terminate', 3.0),\n",
              " ('without limiting', 2.0),\n",
              " ('party may assign', 2.0),\n",
              " ('shall provide written notice', 2.0),\n",
              " ('party may terminate', 2.0),\n",
              " ('either party may', 2.0),\n",
              " ('either party may terminate', 2.0),\n",
              " ('agreement upon written notice', 2.0),\n",
              " ('terminated upon', 2.0),\n",
              " ('providing prior written notice', 2.0),\n",
              " ('licensee upon', 2.0),\n",
              " ('substantial portion', 2.0),\n",
              " ('`', 2.0),\n",
              " ('days prior written notice', 2.0),\n",
              " ('provide services shall', 2.0),\n",
              " ('provide services', 2.0),\n",
              " ('ordinary course', 2.0),\n",
              " ('contract period', 2.0)]"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Algorithm 2\n",
        "\n",
        "**step 1**: extract word with it's keyword\n",
        "\n",
        "(column 1)   ,    column 2\n",
        "\n",
        "(('laws', 'shall', 'prevail'), 'laws')\n",
        "\n",
        "(('laws', 'shall', 'prevail'), 'shall')\n",
        "\n",
        "(('laws', 'shall', 'prevail'), 'prevail')\n",
        "\n",
        "(('laws', 'thereof')，'laws')\n",
        "\n",
        "(('laws', 'thereof'),'thereof')\n",
        "\n",
        "**step 2**: According to step1 column2 groupBy\n",
        "\n",
        "('laws', ('laws', 'shall', 'prevail'), ... , ...)\n",
        "\n",
        "('shall', ('laws', 'shall', 'prevail'),... ,...)\n",
        "\n",
        "column 2: keywords\n",
        "\n",
        "**step 3**: According to step2  rdd.map calculate score(degree/frequency)\n",
        "\n",
        "frequency = len(keywords)\n",
        "\n",
        "degree = frequency + each keywords lens-1(for example: ('laws', 'shall', 'prevail') = 3-1)\n",
        "\n",
        "socreMap = ('laws', score)\n",
        "\n",
        "**step 4**: step1.join(scoreMap).values()\n",
        "\n",
        "(column 1)   ,    column 2\n",
        "\n",
        "(('laws', 'shall', 'prevail'), score)\n",
        "\n",
        "(('laws', 'shall', 'prevail'), score)\n",
        "\n",
        "(('laws', 'shall', 'prevail'), score)\n",
        "\n",
        "(('laws', 'thereof')， score)\n",
        "\n",
        "(('laws', 'thereof'), score)\n",
        "\n",
        "**step 5**: reduceByKey(x + y)\n",
        "\n",
        "**step 6**: sortedBy()"
      ],
      "metadata": {
        "id": "lqJim3Qe6blj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def isPunct(word):\n",
        "  return len(word) == 1 and word in string.punctuation\n",
        "\n",
        "def replace(documents):\n",
        "  phrase_list = []\n",
        "  for document in documents:\n",
        "    sww = sw.words()\n",
        "    document = map(lambda x: \"|\" if x in sww else x,\n",
        "          word_tokenize(document.lower()))\n",
        "    phrase = []\n",
        "    for keywords in document:\n",
        "        if keywords == \"|\" or isPunct(keywords):  \n",
        "          if len(phrase) > 0 and len(phrase) < 5:\n",
        "            phrase_list.append(phrase)\n",
        "            phrase = []\n",
        "          elif len(phrase) > 4:\n",
        "            # phrase_list.append(phrase[0:4])\n",
        "            phrase = []\n",
        "        else:\n",
        "          phrase.append(keywords)\n",
        "    return phrase_list\n",
        "\n",
        "def getWord(phrase_list):\n",
        "  word_list = []\n",
        "  for keyword in phrase_list:\n",
        "    if(len(keyword)< 2):\n",
        "      word_list.append((''.join(keyword),' '.join(keyword)))\n",
        "    else:\n",
        "      for word in keyword:\n",
        "        word_list.append((''.join(word),' '.join(keyword)))\n",
        "  return word_list\n",
        "\n",
        "def countScore(phrase_string_list):\n",
        "  frequency = len(phrase_string_list[1])\n",
        "  degree = 0\n",
        "  for keywords in phrase_string_list:\n",
        "    degree += len(keywords)\n",
        "  score = float(degree / frequency)\n",
        "  return phrase_string_list[0], score\n",
        "\n",
        "def getAllKeywordsFromGoverningLaw (row):\n",
        "  row_dict = row.asDict()\n",
        "  if not row_dict['Governing Law']:\n",
        "    return []\n",
        "  else:\n",
        "    governingLaw = row_dict['Governing Law']\n",
        "    documents = re.split(pattern=\"\\(Page \\d\\)\", string=governingLaw[:-9].strip())\n",
        "    phrase_list = replace(documents)\n",
        "    word_list = getWord(phrase_list)\n",
        "    return [word for word in word_list]\n",
        "\n",
        "def getAllKeywordsFromAntiAssignment (row):\n",
        "  row_dict = row.asDict()\n",
        "  if not row_dict['Anti-assignment']:\n",
        "    return []\n",
        "  else:\n",
        "    antiAssignment = row_dict['Anti-assignment'] \n",
        "    documents = re.split(pattern=\"\\(Page \\d\\)\", string=antiAssignment[:-9].strip())\n",
        "    phrase_list = replace(documents)\n",
        "    word_list = getWord(phrase_list)\n",
        "    return [word for word in word_list]\n",
        "\n",
        "def getAllKeywordsFromChangeOfControl (row):\n",
        "  row_dict = row.asDict()\n",
        "  if not row_dict['Change of Control']:\n",
        "    return []\n",
        "  else:\n",
        "    changeOfControl = row_dict['Change of Control']\n",
        "    documents = re.split(pattern=\"\\(Page \\d\\)\", string=changeOfControl[:-9].strip())\n",
        "    phrase_list = replace(documents)\n",
        "    word_list = getWord(phrase_list)\n",
        "    return [word for word in word_list]"
      ],
      "metadata": {
        "id": "ntncxb3LdJtn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Algorithm 2\n",
        "rddG = spark.read.csv(\"Governing_Law.csv\",header=True).rdd\n",
        "allKeywordsInGoverningLaw = rddG.flatMap(getAllKeywordsFromGoverningLaw)\n",
        "scoreMapG = allKeywordsInGoverningLaw.groupBy(lambda x:x[0]).map(countScore)\n",
        "allKeywordsInGoverningLaw.join(scoreMapG).values().reduceByKey(lambda x,y:x+y).sortBy(lambda x:x[1],ascending=False).take(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7oe64Fu6Wh7",
        "outputId": "724e2a8c-e194-4d66-af12-1fac308e85fa"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('agreement shall', 505.96323215283417),\n",
              " ('laws', 460.98858075041005),\n",
              " ('state', 396.7115384615371),\n",
              " ('governed', 380.7305699481831),\n",
              " ('accordance', 302.9659863945569),\n",
              " ('construed', 269.4801444043303),\n",
              " ('agreement', 196.6723044397472),\n",
              " ('new york', 170.90305257207459),\n",
              " ('without regard', 135.48807854137434),\n",
              " ('without giving effect', 119.47608806963656),\n",
              " ('conflict', 118.46218487394957),\n",
              " ('conflicts', 116.60176991150452),\n",
              " ('law principles', 81.34647887323943),\n",
              " ('interpreted', 70.86764705882354),\n",
              " ('laws principles', 68.53927808285263),\n",
              " ('without reference', 56.431451612903224),\n",
              " ('interpretation', 55.36363636363638),\n",
              " ('internal laws', 55.26771796266087),\n",
              " ('united states', 54.56015037593987),\n",
              " ('california', 53.71428571428576)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rddA = spark.read.csv(\"Anti_assignment_CIC_g3.csv\",header=True).rdd\n",
        "allKeywordsInAntiAssignment = rddA.flatMap(getAllKeywordsFromAntiAssignment)\n",
        "scoreMapAnti = allKeywordsInAntiAssignment.groupBy(lambda x:x[0]).map(countScore)\n",
        "allKeywordsInAntiAssignment.join(scoreMapAnti).values().reduceByKey(lambda x,y:x+y).sortBy(lambda x:x[1],ascending=False).take(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7f5YtWg6Jnv",
        "outputId": "cec3c0d8-650e-4ac8-de9d-4d872358e950"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('prior written consent', 684.0612356031706),\n",
              " ('agreement', 420.5580357142824),\n",
              " ('party', 255.73497267759643),\n",
              " ('rights', 251.68987341772046),\n",
              " ('neither party may assign', 208.11779440165805),\n",
              " ('unreasonably withheld', 183.57647058823545),\n",
              " ('assign', 172.96511627906943),\n",
              " ('obligations', 158.0579710144929),\n",
              " ('transfer', 149.33333333333346),\n",
              " ('agreement without', 138.2112607338017),\n",
              " ('provided', 122.02597402597368),\n",
              " ('third party', 117.3893442622951),\n",
              " ('except', 117.37600000000018),\n",
              " ('either party without', 113.87417561710944),\n",
              " ('obligations hereunder', 113.06235355376216),\n",
              " ('assigned', 110.16521739130427),\n",
              " ('part', 101.88118811881165),\n",
              " ('agreement may', 101.11212073324917),\n",
              " ('assignment', 93.5),\n",
              " ('without', 92.74043715847003)]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "allKeywordsInChangeOfControl = rddA.flatMap(getAllKeywordsFromChangeOfControl)\n",
        "scoreMapCoC = allKeywordsInAntiAssignment.groupBy(lambda x:x[0]).map(countScore)\n",
        "allKeywordsInChangeOfControl.join(scoreMapCoC).values().reduceByKey(lambda x,y:x+y).sortBy(lambda x:x[1],ascending=False).take(20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deTFDjBl7w3E",
        "outputId": "ff47a214-c748-493f-d54e-8d5a481d3617"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('change', 198.66666666666697),\n",
              " ('control', 144.6400000000001),\n",
              " ('agreement', 137.82142857142878),\n",
              " ('occurrence', 102.0),\n",
              " ('closing', 88.0),\n",
              " ('days', 70.0),\n",
              " ('party', 64.4371584699453),\n",
              " ('competing product', 60.0),\n",
              " ('prior written consent', 58.02305123419765),\n",
              " ('circumstances', 56.0),\n",
              " ('terminate', 54.0),\n",
              " ('commercialization', 54.0),\n",
              " ('merger', 49.72972972972972),\n",
              " ('indenture trustee', 49.6),\n",
              " ('consolidation', 48.5714285714286),\n",
              " ('manufacture', 48.0),\n",
              " ('event', 46.06060606060604),\n",
              " ('additional securities representing', 45.0),\n",
              " ('addition', 45.0),\n",
              " ('terminated', 43.333333333333336)]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    }
  ]
}